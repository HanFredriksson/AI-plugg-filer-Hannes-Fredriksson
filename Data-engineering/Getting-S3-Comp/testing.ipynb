{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f802345c",
   "metadata": {},
   "source": [
    "### Labb kod - för att spara till Datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847854ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "from s3pathlib import S3Path\n",
    "import jsonargparse\n",
    "import requests\n",
    "from loguru import logger\n",
    "\n",
    "import log_utils\n",
    "\n",
    "LINK_TO_XML_FILE = {\n",
    "    \"mit\": \"https://news.mit.edu/rss/topic/artificial-intelligence2\",\n",
    "}\n",
    "path = S3Path(\n",
    "    \"s3://data.lake/\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\",\n",
    "    access_key=\"save.load\",\n",
    "    secret_key=\"save.load89\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_metadata_info(blog_name: str) -> str:\n",
    "    assert (\n",
    "        blog_name in LINK_TO_XML_FILE\n",
    "    ), f\"{blog_name=} not supported. Supported blogs: {list(LINK_TO_XML_FILE)}\"\n",
    "    blog_url = LINK_TO_XML_FILE[blog_name]\n",
    "    response = requests.get(blog_url)\n",
    "    xml_text = response.text\n",
    "    # xml_text = response.content.decode(\"utf-8\")\n",
    "    return xml_text\n",
    "\n",
    "\n",
    "def save_metadata_info(xml_text: str, blog_name: str) -> None:\n",
    "    # path_xml_dir = Path(\"data/data_lake\") / blog_name\n",
    "    path_xml_dir = path / blog_name\n",
    "    path_xml_dir.mkdir(exist_ok=True, parents=True)\n",
    "    # with open(path_xml_dir / \"metadata.xml\", \"w\") as f:\n",
    "    #     f.write(xml_text)\n",
    "    # with open(path_xml_dir / \"metadata.xml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     f.write(xml_text)\n",
    "    s3_file = path / blog_name / \"metadata.xml\"\n",
    "    s3_file.write_text(xml_text)\n",
    "\n",
    "def main(blog_name: str) -> None:\n",
    "    logger.info(f\"Processing {blog_name}\")\n",
    "    xml_text = get_metadata_info(blog_name)\n",
    "    save_metadata_info(xml_text, blog_name)\n",
    "    logger.info(f\"Done processing {blog_name}\")\n",
    "\n",
    "\n",
    "def parse_args() -> jsonargparse.Namespace:\n",
    "    parser = jsonargparse.ArgumentParser()\n",
    "    parser.add_function_arguments(main)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     args = parse_args()\n",
    "#     log_utils.configure_logger(log_level=\"DEBUG\")\n",
    "#     main(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "652b3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_name = \"mit\"\n",
    "mit_xml_file = get_metadata_info(blog_name)\n",
    "save_metadata_info(mit_xml_file, blog_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361606f",
   "metadata": {},
   "source": [
    "### Labb kod för att hämta från datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480167b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'log_utils' from 'newsfeed' (f:\\AI-24-programering\\Python-programing-Hannes-Fredriksson\\.venv\\Lib\\site-packages\\newsfeed\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mloguru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnewsfeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m log_utils\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnewsfeed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatatypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlogInfo\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_uuid_from_string\u001b[39m(val: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'log_utils' from 'newsfeed' (f:\\AI-24-programering\\Python-programing-Hannes-Fredriksson\\.venv\\Lib\\site-packages\\newsfeed\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import jsonargparse\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from loguru import logger\n",
    "from s3pathlib import S3Path\n",
    "\n",
    "\n",
    "from newsfeed import log_utils\n",
    "from newsfeed.datatypes import BlogInfo\n",
    "\n",
    "path = S3Path(\n",
    "    \"s3://data.lake/\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\",\n",
    "    access_key=\"save.load\",\n",
    "    secret_key=\"save.load89\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def create_uuid_from_string(val: str) -> str:\n",
    "    assert isinstance(val, str)\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_DNS, val))\n",
    "\n",
    "\n",
    "def load_metadata(blog_name: str) -> BeautifulSoup:\n",
    "    # metadata_path = path / blog_name / \"metadata.xml\"\n",
    "    # # with open(metadata_path) as f:\n",
    "    # #     xml_text = f.read()\n",
    "    s3_file = path / blog_name / \"metadata.xml\"\n",
    "    xml_text = s3_file.read_text()\n",
    "    \n",
    "\n",
    "    parsed_xml = BeautifulSoup(xml_text, \"xml\")\n",
    "    return print(\"Metadata loaded and parsed\"), parsed_xml\n",
    "\n",
    "\n",
    "def extract_articles_from_xml(parsed_xml: BeautifulSoup) -> list[BlogInfo]:\n",
    "    articles = []\n",
    "    for item in parsed_xml.find_all(\"item\"):\n",
    "        raw_blog_text = item.find(\"content:encoded\").text\n",
    "        soup = BeautifulSoup(raw_blog_text, \"html.parser\")\n",
    "        blog_text = soup.get_text()\n",
    "        title = item.title.text\n",
    "        unique_id = create_uuid_from_string(title)\n",
    "        article_info = BlogInfo(\n",
    "            unique_id=unique_id,\n",
    "            title=title,\n",
    "            description=item.description.text,\n",
    "            link=item.link.text,\n",
    "            blog_text=blog_text,\n",
    "            published=pd.to_datetime(item.pubDate.text).date(),\n",
    "            timestamp=datetime.now(),\n",
    "        )\n",
    "        articles.append(article_info)\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "def save_articles(articles: list[BlogInfo], blog_name: str) -> None:\n",
    "    save_dir = Path(\"data/data_warehouse\", blog_name, \"articles\")\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    for article in articles:\n",
    "        save_path = save_dir / article.filename\n",
    "        with open(save_path, \"w\") as f:\n",
    "            f.write(article.json(indent=2))\n",
    "\n",
    "\n",
    "def main(blog_name: str) -> None:\n",
    "    logger.info(f\"Processing {blog_name}\")\n",
    "    parsed_xml = load_metadata(blog_name)\n",
    "    articles = extract_articles_from_xml(parsed_xml)\n",
    "    save_articles(articles, blog_name)\n",
    "    logger.info(f\"Done processing {blog_name}\")\n",
    "\n",
    "\n",
    "def parse_args() -> jsonargparse.Namespace:\n",
    "    parser = jsonargparse.ArgumentParser()\n",
    "    parser.add_function_arguments(main)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    log_utils.configure_logger(log_level=\"DEBUG\")\n",
    "    main(**args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
