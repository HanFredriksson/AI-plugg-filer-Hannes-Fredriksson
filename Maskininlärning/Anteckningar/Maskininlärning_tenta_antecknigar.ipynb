{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anteckningar förberdande för tantan i Maskininlärning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Maskininlärning är tillför att göra förutsägelser och klassifikationer. Vad vi intreserad av mí maskininlärning ar vad vi kan förutsäga med modellen. Inte hur väl den representerar träningsdatan. Det är därför det är så viktigt med testing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "Kan vi köra flera ML modeller mot varandra och hitta vilken som funkar bäst.  \n",
    "Används för att träna modellen och testa datan. Genom att testa och träna med hela data setet i omgångar. Detta för att minimera data leakage\n",
    "Går också att användas till att hitta hyperparametrar. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias - Variance\n",
    "\n",
    "**Bias** - Är vad mycket antaganden som model tar. Mer generliserande, antagande, modellen gör, ger ett högre bias. Det leder till en underfit.  \n",
    "\n",
    "**Variance** - Säger hur känslig modellen är förtränings data settet. Den generilserar inte modellen tillräckligt. Det leder till en overfit. \n",
    "\n",
    "### Lösningar på Hög Bias (Dålig prestanda på tränings data)\n",
    "- Träna modellen mer\n",
    "- Öka complexiteten för modellen\n",
    "- Pröva en annnan modell\n",
    "\n",
    "### Lösning på Hög Variance (Dålig prestanda på validerings data)\n",
    "- Kör på mer tränings data\n",
    "- Use regularization\n",
    "- Byt modell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Är en Generalized Linear Model (GLM), likt Linear Reagression.\n",
    "Kan bara bedöm från ett värde mellan 1 och 0. Den funkar ändå att använda varierande data mellan olika värden inte bara binära värden eller classer. Matten som passar linejn till datan är Maximumlilklelihood. Modellen kan också var användbar att hitta vilke features som är relventa till att klassifiera data punkten.  \n",
    "\n",
    "## Coefficients\n",
    "\n",
    "Om vi använde log odds for logistical regression vi kan använda samma teniker som vi använder för linear regression. \n",
    "\n",
    "## MaximumLiklelihood\n",
    "\n",
    "Även om Logistic Regression är väldigt likt Linear Regression så kan man inte använda sum of least squers för att hitta linjen. Detta är för att - och + är oändlig på y-axeln. Avstånden skulle aldrig konvergera. \n",
    "\n",
    "Vad som händer är att vi transformerar log odds till log likelihood återberäknar linjen för log odds om log liklehood är högre än vad den tidigre linjen för log likehood var.  \n",
    "\n",
    "## Mätvärden\n",
    "\n",
    "**$R^2$** - Strävar vi efter ett så nära värde till 1 sm möjligt för en modell som passar data så bra som möjligt. Det finns inte ett sätt att beräkna detta i Logistic Regression.\n",
    "\n",
    "**p-value** - Som tidigare så är värden under 0.05 signifikanta. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "### Ridge Regression, L2\n",
    "\n",
    "I ridges regression så använder vi bias som ett sätt att minmera risken för en overfit och hög variance. Detta i Jämförellse till en Linear Regerssion modell. Detta bias kallas penelty och är ett värde mellan 0 och posetiv oändligheten. För att hitta det bästa penelty kör en cross validation på modellen. Denna modellen är effektiv när vi har små sett av data på grund av penelty. Kan appliceras på andra modeller för att hitta linjen i data settet. *Penelty ger också möjligheten att lösa för linjen även om data settet inte har fler data punkter än parametrar(Features).* Funkar Bäst när vi vet vad parametrana är och gör. \n",
    "\n",
    "Den optimal lutning för lijen går mot noll ju högre penelty vi sätter, men blir aldrig noll.\n",
    "\n",
    "### Lasso Regression, L1\n",
    "\n",
    "Här också penelty en viktigt del av att passa linjen till data. Fast här så är lutningen vid multiplicering med penelty ett absolut belopp. \n",
    "\n",
    "Om vi ökar penelty så blir tillslut lutning på linjen noll.\n",
    "\n",
    "### Elastic Net Regression\n",
    "\n",
    "Kort så kombinerar Elastic Net panelty för både Lasso och Ridge Regrssion. Denna modellen är kraftfull den kan hitta vilka prarmetrar som är viktiga och vilka som korelaterade. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis - LDA\n",
    "\n",
    "Detta handlar hitta vad som separerar klasserna så mycket som möjligt. Det är likt PCA en teknik att reducer dimnesionerna.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Termer och deras betydelse\n",
    "\n",
    "*- Accuracy* - $\\frac{TP+TN}{TP+TN+FP+FN}$ - Är hur många correct klassifierad delat på totala antalet. generellt värde.\n",
    "\n",
    "*- Recall* -  $\\frac{TP}{TP+FN}$ - Procenten av korrekt klassifierad instanser av alla postiva instanser. Hur många av en klass lyckades modellen klassifiera rätt. \n",
    "\n",
    "*- Precision*  - $\\frac{TP}{TP+FP}$ - Är procenten av antalet korrekt positivt klassifierad av alla positiva instances av klasserna. Så hur många var det som modellen sa var en klass var den klassen. \n",
    "\n",
    "*- F1-value* - $\\frac{2}{(1/Precision)+(1/Recall)}$ - \"Harmonic mean\" av Precision and Score. \n",
    "\n",
    "*- Kostanads funktion* - Är en matimatisk funktion som beräknar felet mellan förutspådda svaret och det faktiska svaret. Som RSME, MSE, Logloss, MSE, MAE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - Models\n",
    "\n",
    "# Supervised learning\n",
    "Vet vi input variabelrna, oberoende variabaler eller features och vi vet output variabeln, target, labels, beroend variabeln. \n",
    "\n",
    "### Regression\n",
    "**Linear Regression** - Förösker hitta en linjär förhållande i data. Genom least sum of squars.  \n",
    "\n",
    "**Logistic Regression** - Klassification modell som förösker hitta sannolikheten en data punkt tillhör en klass. Den funkar bäst i binära förhållanden. Den hittar linjen med maximunliklehood. linjen är s kurva.  \n",
    "\n",
    "**KNN** - Förösker inte hitta en model ekvation, så den behövs inte tränas. Specifikt bra vid en klassifieering när det inte finns någon linjärtförhållande. \n",
    "\n",
    "**SVM Support vectro Machine** - Kan användas vid klassifiering men går att kör på regressioner också. Den förösker dra ett besluts gräns för klasserna. Funnkar mycket bra i högre dimsnioner där det är fler features med litet set av data. Det finns även kernal functions. som kan öka dimensionalliteten i data.\n",
    "Hittar sin linje, hyoerplane, genom att maximera distansen mellan klasserna med hjälp av support vectors. \n",
    "\n",
    "**Naive Bayes Calssifier** - Bra att klassifiera text och ord. \n",
    "\n",
    "**Decision Tree** - en serie av ja och nej frågor. förösker hitta löv som är så rena som möjligt. med bara en klass av data i lövet. Med Descision Tree kan användas i en ensambel algorithm. Där vi kör fler Decision trees åt gången eller i en sekvens. \n",
    "\n",
    "Fungerar att det fin två noder. Decision nodes and Leaf nodes. Decision nodes har ett villkor. Leaf nodes är vad datan blir klassad som. Modellen väljer vilkne split som ska göra genom max infromation gain och försöker ge leaf noderna entropy så lågt som möjligt. Där 0 är helt ren.\n",
    "\n",
    "*Regression* - Den förutser värdet från medlet i leaf noden där den okända datapunkten hamnar. För att hitta vilka splits som funkar bäst i regression. Används \"Varaiance Reduction\", detta mätt mote hela settet och vad spliten producerar. Så vad variancen i spliten minus hela data settet variance. \n",
    "\n",
    "Är mycket känsligt för tränings datan. En sådan här modell ger ofta en hög variance. \n",
    "\n",
    "**Random Forest** - Använder bagging och bootstraping. Där flera träd röstar på vilken klass den okända data punkten har. Random kommer från att den slumpmässigt uteslute olika featuers i data settet. Klara Klassifiering och regression. Tendera vara väldigt robust metod. \n",
    "Modellen använder Bootstraping och aggregation för att skapa slumpmässiga träd. Kort kallat Bagging.\n",
    "Modellen väljer också slumpmässigt ut vilka features som används för varje träd. Går att använda i ett regerssions problem också, likt decicion tree. \n",
    "\n",
    "**Boosted trees** - Löser felat från det tidigar träd och ger ofta ett bättre resultat än vad Random tree ger. Fast kan lätt få en overfit. \n",
    "\n",
    "**Neural Networks** - Hittar sin features själva, dom är dolda från oss. Här lägger vi olika lagar som förösker skapar features för att förutsäga lagret som kommer efter tills vi när våran output. \n",
    "\n",
    "\n",
    "# Unsupervised learning\n",
    "vet vi inte det sanna output variabeln. Letar vi efter kluster och okända klasser som finns. Kan säga vad som är den röda tråden mellan datapunkterna och sammlla dom som har samma rödatråd. Förösker hitta så lika data punkter i varje kluster som möjligt. \n",
    "\n",
    "**K-means klustering** - Förösker du hitta center i kluster.\n",
    "\n",
    "## Dimensonality reduction\n",
    "\n",
    "**PCA** - PCA används för konvertera korelationer eller avsaknad mellan alla features i två dimnasioner. Kan användas för att baka ihop featurs till en ny features eller plocka bortdom om dom inte tillför något.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
