{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kort om vad vi gjort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linjär Regression\n",
    "\n",
    "EDA så gör vi det mest för oss själva. Det är mer skapa egenförståelse för vad data ser ut. Är inget man direkt presenterar. Är en undröskning för att se hu värdena ser ut, fördelningen och medel. \n",
    "\n",
    "Korrelationer är något generlet att man ska ta bort. Mattimatiskt är det tveksamt. För att det finns intraktions effekt och det tyder på att featuers på verkar varandra. Effekt om korraltion är stark mellan två features är att man får ett bias. Kan inte längre lita på statistikor som signifikans tester. \n",
    "\n",
    "Multi linjär regression är beta_hat en viktning. Det är dyrt att lägga till mer features och billigt med data när man använder singular value decomposition. \n",
    "\n",
    "### MAE - Mean Absolut Error\n",
    "\n",
    "Fördelen är att den ger samma enhet som in datan. Här underskattar vi felen. Det beror på att vi inte tar hänsyn till variansen här\n",
    "\n",
    "### MSE - Mean Squared Error\n",
    "\n",
    "Här så tar vi hänsyn till variansen och felan kan bli större skilland på små och stora fel. \n",
    "\n",
    "### RMSE \n",
    "\n",
    "Detta straffar större fel och ger oss svar i enheter som in datan och är lättare att tolka. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Train-test split\n",
    "\n",
    "Här gäller det verkligen tänka på att inte får data leakage. Gäller att tänka på när resultaten blir för bra för att vara sant. Det kan var en kolumn som åker med som inte ska vara med. Enklast vad som kan hända är att data punkter åker med i spliten. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomiel regression\n",
    "\n",
    "Polynom är linjära mot varandra. Vi utnyttjar linjär avbildning. Är metoden vi använder på polynomne som gör dom linjära. Det är lätt att overfit en polynomile regression, använder för många degrees of fit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course vs. Blessing of Dimensionality\n",
    "\n",
    "**Curse** I många dimensioner blir avståndet större. Beräknings kostnanden går upp med fllera dimansioner. \n",
    "\n",
    "**Blessing** Vi kan linjarisera väldigt icke-linjära förhållanden i högre dimansioner. \n",
    "\n",
    "Gennerellt vill vi ha små modeller med hög $R^2$ / låg MSE är att föredra, då de generalisera bättre (dvs säga bättre på okänd data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Forward selection\n",
    "\n",
    "kör p-st regerssioner. Välj den med hägst  $R^2$ / lägst MSE. I detta fallet för \"Forward Selection\" rä $R^2$ ett bättre mått. Testa resterande P-1 variabler. Lägg till om $R^2$ / MSE,  ökar/minskar. Se upp för kolinjäritet, för då gäller inte $R^2$/MSE längre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward elimiation \n",
    "\n",
    "Börjar med alla p-variabler utvärdera. Testa alla modeller med p-1 variabler. Fortsätt tills $R^2$/MSE blir sämre. \n",
    "\n",
    "Den är särskillt känslig för multi kolinjäritet. (Som dessutom är svåra att upptäcka!)\n",
    "\n",
    "Men om man verkligen har oberoende variabler, så är den mycket tillförlitilig. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - Principal Component Analysis\n",
    "\n",
    "Grundiden är att hitta en ortogonalbas till designmatrisen- dvs hitta en minmal, obreoende uppsättning variabler. \n",
    "\n",
    "Du skapar en bas bytes matris genom att skriva upp varje variabel som en linjärkombination av alla andra variabler. I den matris vi skrivit upp finns det ett maximalt oberoende variabler i matrisen. Väljer man då $phi$ har störrst varians. För att deta ska funka så måste den var standard normal. \n",
    "\n",
    "Den lär sig inget här. \n",
    "\n",
    "Oövervakad inlärning - \"Vi lär oss om kolinjärietet i systemet\" Detta på feature mängden. Den behöver köra först innan en model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "Grundide: Minimera intrakluster variation. Hitta mängder där punkterna är som nära varandra som möjligt. Här använder vi l2-norm. Går också att byta till l1-norm eller cosinus-likhet. \n",
    "\n",
    "Vanligt är att vi kär Logistic Regression med klustrnena som klasser. Eller en SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-problem\n",
    "\n",
    "I allmänhet är ett ML-Problem ett optimeringsproblem vi optimerar en kostnadsfunktion. \n",
    "Som RSS, MSE\n",
    "\n",
    "Om vi har bias dominerar över bruset men vi har låg varians. En underfit. \n",
    "Minska varians geno Reguljärisering. Med L2-norm. Ridge Regerssion. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
