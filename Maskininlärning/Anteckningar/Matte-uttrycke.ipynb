{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mattematiskmanalys\n",
    "\n",
    "$f(x)$ - en function med \"x\" som parameter. Nekelvärd, dvs $f(x)-X^2$ är i allmänhet **inte** en funktion. För det ger $\\pm$ som svar, den har inte ett enskillt värde. \n",
    "\n",
    "Om vi begänsar oss till bara + eller - så är $f(x)$\n",
    "\n",
    "Om vi vill ha flera variablar för vi göra en fler variabrl analys.\n",
    "\n",
    "$x^2 + y^2 = 1$ \n",
    "\n",
    "$f(x,y) = x^2 + y^2 -1$\n",
    "\n",
    "$f(x, y, z, w) = xz+yw-1$\n",
    "\n",
    "x=z och y=w = $x^2 + y^2 -1$ \n",
    "\n",
    "### Derivering\n",
    "Den är linjär, hät minskar vi dimonsionen när vi deriverar. konstanten försvinner här.  \n",
    "\n",
    "y = kx+m\n",
    "\n",
    "$y' = \\frac{d}{d_x} y$\n",
    "\n",
    "$\\frac{d}{d_x} ax^n = n\\times a \\times x^{n-1}$\n",
    "\n",
    "$\\frac{d}{d_x}y = kx^0 + 0 = k$\n",
    "\n",
    "$\\frac{d}{d_x}x^2 = 2x^2$\n",
    "\n",
    "Bevist för derivatan är att minska streckan mellan två punkter i ett diagram. \n",
    "\n",
    "### Integrering (Prmitiv funktion)\n",
    "Den är affin. Är ökar vi dimonsionen. För det till kommer ett bias.\n",
    "\n",
    "$\\int{x^2} \\to \\frac{x^3}{3} + C$\n",
    "\n",
    "^^^^ Allt detta gäller bar i en dimnasion ^^^^\n",
    "\n",
    "\n",
    "# Derivat i flera dimnesioner\n",
    "\n",
    "$f(x_1, x_2,.... x_n)$\n",
    "\n",
    "Den kallas en Gradient (Fler variabels derivat eller diffrensial)\n",
    "\n",
    "$\\Delta f(x, y)$\n",
    "\n",
    "Partiell derivata: $\\frac{\\delta}{\\delta x} f(x_1, x_2)$\n",
    "\n",
    "\n",
    "### För ML\n",
    "\n",
    "Då försöker vi hela tiden hitta ett hyperplan som sepparerar i olika klasser.\n",
    "Antinger är regressions plan som ett medel eller vad som kan reprenstner medlet med ett hyperplan.\n",
    "Vad vi kan använda är en stokastik nedåtgående grandient, Stockastic Gradient Descent. Vad vi gör är att ta slumpmässiga steg i nedåt gående.\n",
    "\n",
    "För att hitta regressions linjen kan vi använda STG istället för sum of least squers. \n",
    "Hur STG funkar är at vi tar ett litet steg efter gradiaten. Den går efter att signifikanten blir högre. Stoppet blir när man ser att värdet på signifikansen ändras mycket lite. Den går öändligt annars. En begränsing är att man låter den ta mindre steg efter varje steg. \n",
    "Eller ger man den ett momentum (ADAM). \n",
    "\n",
    "\n",
    "Problematiken är att du får inte samma reusltat för varje gång man kör algoritmen, SGD. Det går heller inte reproducerad resultaten. Så det finns ingen vetenskaplig grund till relutat in en ML's. Sammasak kan man inte använda modellen som vetenskap. För att modellen är unik också.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
