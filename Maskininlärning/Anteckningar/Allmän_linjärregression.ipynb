{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allmän linjär regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almänt om ML och linjär regression \n",
    "\n",
    "ML handlar mycket om att representera allmän kunskap numersikt, vikterna i en matris, vikterna är beta. Vad vi gjort så här läng multipel linjär regression. Vad vi söker lonjäer approximationer givet ett stickprov(träningsdata). Om vi mäter i en specefik punkt av datan så kan vi använda local regression. \n",
    "\n",
    "Vad vi försöker lösa är $beta_0  beta_1$ och bestäma. \n",
    "\n",
    "Detta var våran optimering för våran linjen, SSE,  Sum of square errors. Vi kan även optimera med MSE. \n",
    "$\\min C(B) \\sum_{n}^{i=1}(y_i-\\beta_0x_i)^2 = \\sum_{n}^{i=1}(y_i-\\hat{y})^2$\n",
    "\n",
    "Hur optimerar vi då. Man sätter derivatan till noll och analyserar resultaten. $\\epsilon$ finns alltid kvar även om den är onsynlig i ekvationen. Är det icke reducerbara felet. Så vad vi göra är att då optimera för låg varians. Väntvärdet för Y givet X är linjärt beroande till X $E[Y|X] = XB$. Detta vad vi antar när vi kör en linjär regression. \n",
    "\n",
    "### Fler dimansioner\n",
    "\n",
    "Lutningen på planet är inte längre en skaler, det är nu en gradient. Grandienten pekar alltid i den rikting där förändringshastigheten är störst. Planet blir en saddel punkt. Då är inte förhållandet för Väntvärdet för Y givet X längre linjärt. Det blir då en funktion. Detta är vad som händer när vi lägger till features i väran regression, feature engineering. $E[Y|X] = f(X)$ I boken är så refrerar man till kolinjeritet förmycket. Vad som kolinjäritet visar bara att den aviker från signifikansen.\n",
    "\n",
    "Det är dyrara att använda fler dimonsioner i en model. För kostanden per dimension är ^2. Där blir en polynom-expansion väldigt dyr. För Dimonsioner öker väldigt snabbt. \n",
    "\n",
    "## Bias och varianse \n",
    "\n",
    "Om vi säger att bias är noggrannhet och variansen är precision. Så med hög varainse och bias så är det mest brus. \n",
    "\n",
    "![alt text](Precision-versus-accuracy-The-bullseye-represents-the-true-value-eg-the-true.png)\n",
    "\n",
    "## Generell Additiv Modell \"Kolla kap7 i boken.\" \n",
    "\n",
    "$Y = f_0 (x_0) + f_1(x_1) +....+ f_d(x_d)(+\\epsilon) \\to \\sum_{i=1^n} x_0f_0(x_0) \\to \\sum_{i=1^n} x_0 f(X)$ \n",
    "\n",
    "Här förutsäger vi Y med funktioner för X så vi kan lösa Y. Slår vi ihop alla funktionerna så för vi en marginell densitet. om vi väjer f som en distribution, dår är det en sannolikhets distribution. Om den då är en normal distribution så får vi en linjärt samband för fuktionen.  \n",
    "\n",
    "Nu är det sannolikheten intressant istället för närmvärdet. \n",
    "\n",
    "Viktigt koncept för allmän regression är **länk funktioner** . Om det klassfiersings problem är det bernoulli problem. \n",
    "\n",
    "## Kolinjäritet\n",
    "\n",
    "Nu finns det en koljinäritet för att vi har funktioner för att bestäma Y. Så nu behöver vi ortogonalisera våra variabler som gör gör dom lijärt obreoande. Så att hitta en riktig bas. \n",
    "Regualisering är en teknik för att hitta ett oberoende. Detta gör vi på datan. \n",
    "\n",
    "### Regularisering\n",
    "\n",
    "Den minskar variansen men ökar bias. \n",
    "- underfit: Låg varians, högt bias(hög precision, låg noggranhet) och stort konfidensintervall, låg noggranhet\n",
    "- overfit: Hög varians, lågt bias(låg precision, hög noggranhet)\n",
    "Man får högre bias om vi regulasera. Är inte heller så känslig för outliers. \n",
    "\n",
    "\n",
    "$\\min C(B) = RSS + \\lambda\\sum_{i=1}^d \\beta^2_d$ - $L_2$ norm. \n",
    "\n",
    "$\\lambda$ - hyperparametar, hur stark effekten, tal mellan 0 och 1. Hur mycket åt regression vill vi gå och vi försöker hitta vilken grad den ska vara. \n",
    "\n",
    "Vad som händer är att vi straffar modellen för att ha stora värden på koeficienterna. Så vi får små $\\beta$ så att utslagen blir små på Y, låg varianse. Så den ser till att den födelar alla värdena jämnt över alla koeficienterna. Kallas Ridge regression. Så om vissa koeficient inte är signifikanta så är detta problematiskt. \n",
    "\n",
    "$\\min C(B) = RSS +  \\lambda\\sum_{i=1}^d |\\beta_d|$ - $L_1$ norm - Lasso regression. Den här tenderar att sätta $\\beta$ till noll om dom inte signifikanta. Det har med absolutbelloppet $|\\beta_d|$.\n",
    "\n",
    "Vad vi vill gör att kunna köra både Lasso regression och Ridge regression. Kallas Elastic net regression.\n",
    "\n",
    "$\\min C(B) = RSS + \\lambda(\\frac{1-\\alpha}{2} \\sum_{i=1}^{d} \\beta^2_i + \\alpha\\sum_{i=1}^{d}|B_i|)$\n",
    "\n",
    "$\\alpha$ - bestämer balansen mellan Lasso- och Ridge regression. Värde mellan 0 och 1. \n",
    "\n",
    "För att vi skall kunna kör en regularisering så måste data setet vara standardiserat. För att hitta minsta av något kan var väldigt olika beroende på enheterna i datan, ex, om det km och meter i två olika variabler så måste vi ha standardiserat för att få samma värden.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skalering\n",
    "\n",
    "- Min/max skalering, mycket känslig för outliers, kan tänka att man centrerar kring nollan\n",
    "- Standardisering, skapar en standard normal distrubution, $\\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "Varför vill vi gör en skalering, det handlar om att data punkterna är i data är av enheter dem väldigt olika värden. Då kan vi skalera datan så vi får ett bättre resulatat av våra regressioner. Vid regularisering så är det ett måste att skalera. Om vill ha ett värde som reprsenterar 0, inget, om det inte finns. Så är det viktigt att skalera så vi får en representant för vad som är 0.\n",
    "\n",
    "När vill vi inte skalera. Y skaleras mycket sällan. Handlar om vi behålla utfallet av Y. Om inte Y är sannolikheter, för vi vill att de ska summera till 1.\n",
    "\n",
    "Bra att tänka på om det behövs en sklaering är om test värdena visar nonsens eller väldigt dålig värden. Oftas så säger vilken metod man vill använda om vi behöver skalering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Minsta kvadrat metoden och maximum-likelihood\n",
    "\n",
    "Är ett specialfall av maximum-likelihood metoden. Minsta kvadrat metoden kan ses som den klassifierar den mest sannolika medlet.  \n",
    "\n",
    "$\\max C(B) = \\Pi p \\Pi 1-p$ - en klass\n",
    "\n",
    "Nu funkar inte formeln för våra variabler, $\\beta$, som vi hade för linjär regression. När vi börjar generaliserad så finns det inte längre några analytiska lösningar. Det går inte alltid här leda med formler. Vi ger också upp nomral distributation och linjärirtet gäller inte längre. Så vi måste hitta andra sätt att optimer våran kostanad funktion. \n",
    "\n",
    "## Gradiant descent\n",
    "\n",
    "Detta är lösning när vi har generaliserat. Det är iterativ metoder. Den har ett stop villkor och varje iteration kallas en epok. Det finns en stor risk för att bli kaotiska. Dom bryter ihop till kaos vid 3.58. Dom behöver också ett start värde för att fungera. Den blir också olika varje gång vi kör den. Vad vi förösker göra här är att styra bruset, kaoset, i någon mening med gradient descent. Fördelen med dom här metoderna är att dom väldigt billliga att köra. Så om det går snet gör inte så mycket.   \n",
    "\n",
    "![alt text](../Matrial/Screen-Shot-2018-02-24-at-11.47.09-AM.png)\n",
    "\n",
    "- Steg längden är viktigt för att hittat minmum punkten\n",
    "- Vi beräknar på alla punkterna på en gång. Om då ytan då är ojämn så hoppar vi omkring en massa. ger inget bra resultat.\n",
    "\n",
    "Lösing på dom här problem är:\n",
    "\n",
    "**SQD** - Stochastic gradient descent, väljer en slumpmässig punkt. Nu kommer inte metoden att stanna om vi inte har ett stop villkor. Fördelen den fastanr inte lokala minimum punkter. För den går inte efter vart gradient lutar brantast. \n",
    "\n",
    "För **SQD** så behöver vi justera några parametrar för att SQD ska bli bra. \n",
    "- Steglängden bör förändras, Adaptive Gradiant descent.\n",
    "- För att undvika att fastna i lokal minimum, så ger den ett momentum. Kallas ADAM, Adam Adaptive Moment Estimation.\n",
    "\n",
    "**Mini-batch gradient descent** - Så väljer vi en slumpmässig delmängd och räknar gradient på. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
