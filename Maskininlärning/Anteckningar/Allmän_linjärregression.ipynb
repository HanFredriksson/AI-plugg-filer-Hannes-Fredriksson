{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allmän linjär regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almänt om ML och linjär regression \n",
    "\n",
    "ML handlar mycket om att representera allmän kunskap numersikt, vikterna i en matris, vikterna är beta. Vad vi gjort så här läng multipel linjär regression. Vad vi söker lonjäer approximationer givet ett stickprov(träningsdata). Om vi mäter i en specefik punkt av datan så kan vi använda local regression. \n",
    "\n",
    "Vad vi försöker lösa är $beta_0  beta_1$ och bestäma. \n",
    "\n",
    "Detta var våran optimering för våran linjen, SSE,  Sum of square errors. Vi kan även optimera med MSE. \n",
    "$\\min C(B) \\sum_{n}^{i=1}(y_i-\\beta_0x_i)^2 = \\sum_{n}^{i=1}(y_i-\\hat{y})^2$\n",
    "\n",
    "Hur optimerar vi då. Man sätter derivatan till noll och analyserar resultaten. $\\epsilon$ finns alltid kvar även om den är onsynlig i ekvationen. Är det icke reducerbara felet. Så vad vi göra är att då optimera för låg varians. Väntvärdet för Y givet X är linjärt beroande till X $E[Y|X] = XB$. Detta vad vi antar när vi kör en linjär regression. \n",
    "\n",
    "### Fler dimansioner\n",
    "\n",
    "Lutningen på planet är inte längre en skaler, det är nu en gradient. Grandienten pekar alltid i den rikting där förändringshastigheten är störst. Planet blir en saddel punkt. Då är inte förhållandet för Väntvärdet för Y givet X längre linjärt. Det blir då en funktion. Detta är vad som händer när vi lägger till features i väran regression, feature engineering. $E[Y|X] = f(X)$ I boken är så refrerar man till kolinjeritet förmycket. Vad som kolinjäritet visar bara att den aviker från signifikansen.\n",
    "\n",
    "Det är dyrara att använda fler dimonsioner i en model. För kostanden per dimension är ^2. Där blir en polynom-expansion väldigt dyr. För Dimonsioner öker väldigt snabbt. \n",
    "\n",
    "## Bias och varianse \n",
    "\n",
    "Om vi säger att bias är noggrannhet och variansen är precision. Så med hög varainse och bias så är det mest brus. \n",
    "\n",
    "![alt text](Precision-versus-accuracy-The-bullseye-represents-the-true-value-eg-the-true.png)\n",
    "\n",
    "## Generell Additiv Modell \"Kolla kap7 i boken.\" \n",
    "\n",
    "$Y = f_0 (x_0) + f_1(x_1) +....+ f_d(x_d)(+\\epsilon) \\to \\sum_{i=1^n} x_0f_0(x_0) \\to \\sum_{i=1^n} x_0 f(X)$ \n",
    "\n",
    "Här förutsäger vi Y med funktioner för X så vi kan lösa Y. Slår vi ihop alla funktionerna så för vi en marginell densitet. om vi väjer f som en distribution, dår är det en sannolikhets distribution. Om den då är en normal distribution så får vi en linjärt samband för fuktionen.  \n",
    "\n",
    "Nu är det sannolikheten intressant istället för närmvärdet. \n",
    "\n",
    "Viktigt koncept för allmän regression är **länk funktioner** . Om det klassfiersings problem är det bernoulli problem. \n",
    "\n",
    "## Kolinjäritet\n",
    "\n",
    "Nu finns det en koljinäritet för att vi har funktioner för att bestäma Y. Så nu behöver vi ortogonalisera våra variabler som gör gör dom lijärt obreoande. Så att hitta en riktig bas. \n",
    "Regualisering är en teknik för att hitta ett oberoende. Detta gör vi på datan. \n",
    "\n",
    "### Regularisering\n",
    "\n",
    "Den minskar variansen men ökar bias. \n",
    "- underfit: Låg varians, högt bias(hög precision, låg noggranhet) och stort konfidensintervall, låg noggranhet\n",
    "- overfit: Hög varians, lågt bias(låg precision, hög noggranhet)\n",
    "Man får högre bias om vi regulasera. Är inte heller så känslig för outliers. \n",
    "\n",
    "\n",
    "$\\min C(B) = RSS + \\lambda\\sum_{i=1}^d \\beta^2_d$ - $L_2$ norm. \n",
    "\n",
    "$\\lambda$ - hyperparametar, hur stark effekten, tal mellan 0 och 1. Hur mycket åt regression vill vi gå och vi försöker hitta vilken grad den ska vara. \n",
    "\n",
    "Vad som händer är att vi straffar modellen för att ha stora värden på koeficienterna. Så vi får små $\\beta$ så att utslagen blir små på Y, låg varianse. Så den ser till att den födelar alla värdena jämnt över alla koeficienterna. Kallas Ridge regression. Så om vissa koeficient inte är signifikanta så är detta problematiskt. \n",
    "\n",
    "$\\min C(B) = RSS +  \\lambda\\sum_{i=1}^d |\\beta_d|$ - $L_1$ norm - Lasso regression. Den här tenderar att sätta $\\beta$ till noll om dom inte signifikanta. Det har med absolutbelloppet $|\\beta_d|$.\n",
    "\n",
    "Vad vi vill gör att kunna köra både Lasso regression och Ridge regression. Kallas Elastic net regression.\n",
    "\n",
    "$\\min C(B) = RSS + \\lambda(\\frac{1-\\alpha}{2} \\sum_{i=1}^{d} \\beta^2_i + \\alpha\\sum_{i=1}^{d}|B_i|)$\n",
    "\n",
    "$\\alpha$ - bestämer balansen mellan Lasso- och Ridge regression. Värde mellan 0 och 1. \n",
    "\n",
    "För att vi skall kunna kör en regularisering så måste data setet vara standardiserat. För att hitta minsta av något kan var väldigt olika beroende på enheterna i datan, ex, om det km och meter i två olika variabler så måste vi ha standardiserat för att få samma värden.   \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
