{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3cd3daa",
   "metadata": {},
   "source": [
    "\n",
    "# ‚úÖ Anteckningar inf√∂r tentamen i Maskininl√§rning\n",
    "\n",
    "## Vad √§r maskininl√§rning?\n",
    "Maskininl√§rning handlar om att skapa modeller som kan f√∂ruts√§ga (regression) eller klassificera (klassificering) data. Vi √§r inte intresserade av hur v√§l modellen passar tr√§ningsdata, utan hur v√§l den generaliserar till ny, osedd data. D√§rf√∂r √§r testdata och validering viktigt.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Modellvalidering och prestanda\n",
    "\n",
    "### Cross-Validation\n",
    "Korsvalidering delar upp datan i flera delar (t.ex. k-faldig) d√§r modellen tr√§nas p√• vissa delar och testas p√• andra. Hj√§lper till att minska √∂veranpassning (overfitting) och uppskatta modellens generaliseringsf√∂rm√•ga.\n",
    "\n",
    "### Bias - Varians\n",
    "- **Bias** ‚Äì Hur mycket modellen f√∂renklar problemet. H√∂g bias ger underfitting.\n",
    "- **Variance** ‚Äì Hur k√§nslig modellen √§r f√∂r sm√• f√∂r√§ndringar i tr√§ningsdata. H√∂g variance ger overfitting.\n",
    "\n",
    "**L√∂sning vid h√∂g bias**:\n",
    "- Tr√§na l√§ngre\n",
    "- Anv√§nd mer komplex modell\n",
    "- L√§gg till fler features\n",
    "\n",
    "**L√∂sning vid h√∂g variance**:\n",
    "- Mer tr√§ningsdata\n",
    "- Anv√§nd regularisering\n",
    "- Anv√§nd enklare modell\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Viktiga utv√§rderingsm√•tt\n",
    "\n",
    "| M√•tt | Formel | Beskrivning |\n",
    "|------|--------|-------------|\n",
    "| **Accuracy** | TP+TN / (TP+TN+FP+FN) | Andel korrekt klassificering |\n",
    "| **Precision** | TP / (TP+FP) | Andel korrekt positiva av alla som klassades som positiva |\n",
    "| **Recall (Sensitivity)** | TP / (TP+FN) | Andel korrekt identifierade positiva av alla verkligt positiva |\n",
    "| **F1-score** | 2 / (1/Precision + 1/Recall) | Harmoniskt medelv√§rde av precision och recall |\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ Kostnadsfunktioner\n",
    "Anv√§nds f√∂r att m√§ta hur l√•ngt modellen √§r fr√•n sanningen.\n",
    "\n",
    "- **MSE (Mean Squared Error)** ‚Äì vanlig i regression, straffar stora fel.\n",
    "- **MAE (Mean Absolute Error)** - Valig i regression, straffar inte stora fel lika h√•rt. \n",
    "- **LogLoss (Cross-Entropy Loss)** ‚Äì vanlig i klassificering. Straffar fel som har stort f√∂rtroende. Baserat p√• sannolikhet. \n",
    "- **RMSE (Root Mean Squared Error)** ‚Äì kvadratroten av MSE\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Supervised Learning\n",
    "\n",
    "Vi vet b√•de input (features) och output (labels).\n",
    "\n",
    "### üî∑ Regression\n",
    "**Linear Regression**  \n",
    "- Passar en linje till data genom att minimera **Sum of Squared Errors (SSE)**.\n",
    "- R^2 anv√§nds som m√•tt p√• modellens f√∂rklaringsgrad.\n",
    "\n",
    "**Ridge Regression (L2)**  \n",
    "- L√§gger till en **straffterm (Œª Œ£ Œ≤¬≤)** f√∂r att minska variansen. Bra f√∂r sm√• dataset med multikollinearitet. Koeficienterna blir inte noll. S√• alla features blir kvar. \n",
    "\n",
    "**Lasso Regression (L1)**  \n",
    "- Straffar med **absolutbelopp (Œª Œ£ |Œ≤|)**. Kan s√§tta koefficienter till exakt noll ‚Äì bra f√∂r feature selection.\n",
    "\n",
    "**Elastic Net**  \n",
    "- Kombination av L1 och L2. Hanterar b√•de multikollinearitet och feature selection.\n",
    "\n",
    "**Logistic Regression**  \n",
    "- En klassificeringsmodell som skattar sannolikheten att en datapunkt tillh√∂r en klass.\n",
    "- Anv√§nder **maximum likelihood**, inte SSE.\n",
    "- Output mellan 0 och 1.\n",
    "- Vanlig vid bin√§r klassificering.\n",
    "\n",
    "### üî∑ Klassificering\n",
    "\n",
    "**K-Nearest Neighbors (KNN)**  \n",
    "- Ingen modell tr√§nas. Klassificerar baserat p√• n√§rmaste grannar.\n",
    "- Bra n√§r det inte finns linj√§ra samband.\n",
    "\n",
    "**Support Vector Machines (SVM)**  \n",
    "- Hittar det hyperplan som maximerar marginalen mellan klasser.\n",
    "- Fungerar bra i h√∂ga dimensioner.\n",
    "- **Kernel Trick** anv√§nds f√∂r att hantera icke-linj√§ra separationer utan att uttryckligen √∂ka dimensionsantalet ,likt polynomial expansion, men utan straffet av polynimial explsion. D√• vi inte √§ndra features i data settet. \n",
    "\n",
    "**Naive Bayes**  \n",
    "- Bygger p√• Bayes sats + antagande om oberoende mellan features.\n",
    "- V√§ldigt snabb. Bra f√∂r textklassificering.\n",
    "\n",
    "**Decision Tree**  \n",
    "- Tr√§det byggs genom att maximera **information gain**.\n",
    "- Leaf nodes inneh√•ller klass.\n",
    "- Risk f√∂r √∂veranpassning.\n",
    "- Vid regression s√• Varaiance Reduction. \n",
    "\n",
    "**Random Forest**  \n",
    "- Ensemble av tr√§d, varje tr√§d tr√§nas p√• ett bootstrappat sample.\n",
    "- Robust, minskar variance.\n",
    "\n",
    "**Boosting (t.ex. Gradient Boosting)**  \n",
    "- Bygger tr√§d sekventiellt d√§r varje nytt tr√§d f√∂rb√§ttrar det f√∂rra.\n",
    "- Risk f√∂r overfitting om inte reglerat.\n",
    "\n",
    "**Neurala n√§tverk**  \n",
    "- Lagrar flera lager av noder som f√∂rs√∂ker extrahera m√∂nster.\n",
    "- \"Black box\", men kraftfull vid stora dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Unsupervised Learning\n",
    "\n",
    "Ingen k√§nd output. M√•l: hitta struktur.\n",
    "\n",
    "**K-means Clustering**  \n",
    "- Delar in data i k kluster baserat p√• avst√•nd till klustercentroid.\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Dimensionalitetsreduktion\n",
    "\n",
    "**Principal Component Analysis (PCA)**  \n",
    "- Transformerar datan till nya ortogonala komponenter som f√∂rklarar mest varians.\n",
    "- Anv√§nds f√∂r att minska antalet features.\n",
    "\n",
    "**Linear Discriminant Analysis (LDA)**  \n",
    "- Lik PCA men maximerar separationen mellan klasser.\n",
    "- Kan ocks√• anv√§ndas f√∂r klassificering.\n",
    "\n",
    "---\n",
    "\n",
    "## üìè KoefÔ¨Åcienten och signifikans\n",
    "\n",
    "- I logistisk regression √§r koefficienterna i **log-odds**.\n",
    "- Med exponentiering f√•r vi **odds-ratio**.\n",
    "- **p-v√§rde < 0.05** inneb√§r att koefficienten √§r signifikant.\n",
    "\n",
    "---\n",
    "\n",
    "## Kernel Trick vs Polynomial Expansion (f√∂r SVM)\n",
    "\n",
    "- **Polynomial Expansion**: explicit skapar nya features som t.ex. x1¬≤, x1x2, etc.\n",
    "- **Kernel Trick**: implicit ber√§knar likheten i en h√∂gre dimension utan att skapa nya features. Effektivt vid komplexa separationer.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
