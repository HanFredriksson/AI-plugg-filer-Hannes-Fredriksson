{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for kap 3 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear regression\n",
    "\n",
    "This is an method to predict an value for Y with an single variable X. It assumes an linear realtionship between X and Y. \n",
    "\n",
    "Mathamatically: $Y \\approx \\beta_0 + \\beta_1X$\n",
    "\n",
    "Where(coefficients or parameters): \n",
    "- **$\\beta_0$** - intercept\n",
    "- **$\\beta_1$** - slope\n",
    "\n",
    "Fomula for the prediction:\n",
    "\n",
    "$\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x$\n",
    "\n",
    "To find the **$\\beta_0$**(intercept) **$\\beta_1$**(slope). Residual sum of squares: \n",
    "\n",
    "$\\beta_1 = \\frac{(\\sum^n_{i=1}​(y_i​ − \\overline{y}))(\\sum^n_{i=1}​(x_i​ − \\overline{x}))}{\\sum^n_{i=1} (x_i-\\overline{x})(Y-\\overline{y})}$\n",
    "\n",
    "$\\beta_0 = \\overline{y} - \\hat{\\beta}\\overline{x}$\n",
    "\n",
    "Where: \n",
    "- $\\overline{x}, \\overline{y}$ is the sample mean. \n",
    "\n",
    "Ther is an unreducable error in for how this function works to estimate the Y value, called $\\epsilon$. This error is due to non linear realtionships, measuring errors or othe couses for variations in Y. We do not know the value or why this is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the Accuracy of the Coefficient Estimates\n",
    "\n",
    "What we need to know is how foar is the estimated mean ($\\hat\\mu$) from the true mean ($\\mu$). We know that given an large enough data sets will give the true mean. But given a signle point, how far off is the estimeted mean? We find this through *standarad error of the mean*\n",
    "\n",
    "$Var(\\hat{\\mu}) = SE(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n}$\n",
    "\n",
    "Ingeneral, σ2 is not known, but can be estimated from the data. This estimate of σis known as the residual standard error, and is given by the formula *residual standard error*.\n",
    "\n",
    "$RSE = \\sqrt{RSS/(n −2)}$\n",
    "\n",
    "With standar errors we can find the confidens intrevall. Wich is the percentag of probabilty of the range will contain the true unknown values. So if we have an 95% confidence interval for $\\beta_1$, we know that 95% of the estmiates from $\\beta_1$ is within range of the true value.\n",
    "\n",
    "Formula:  $\\beta_1 \\pm2*\\sum^n_{i=1}​(x_i​ − \\overline{x})^2$ \n",
    "\n",
    "Standard errors can also be used to perform hypothesis tests on the hypothesis\n",
    "testcoefficients. The most common hypothesis test involves testing the null hypothesis of null\n",
    "hypothesis\n",
    "\n",
    "$H_0$ : There is no relationship between X and Y (3.12)\n",
    "\n",
    "versus the alternative hypothesis alternative hypothesis\n",
    "\n",
    "$H_a$ : There is some relationship between X and Y . (3.13)\n",
    "    \n",
    "Mathematically, this corresponds to testing\n",
    "\n",
    "$H_0$ : β1 = 0 \n",
    "\n",
    "versus\n",
    "\n",
    "$H_a$ : β1 %= 0\n",
    "\n",
    "In the case of $\\beta_1 = 0$ the model would reduce to $Y = \\beta_0 +\\epsilon$ and there is no relationshiå between X and Y. We can assume $beta_1$ is far from zero depanding on the the standard error of the predictor ($\\beta_1$) if ther value is small ther is a strong sugestion that $\\beta_1$ is not zero. In order to determin that $\\beta_1$ is not zero we use  t-statistic given by:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "t = \\frac{\\hat{\\beta_1} - \\mathbb{0}}{SE(\\hat{\\beta_1})}\n",
    "\\end{equation*}\n",
    "\n",
    "The t-statistic is used in hypothesis testing to determine whether a regression coefficient $\\beta$ is significantly different from zero. If we se an small p-value from this, smaller then 5%, 0.05, we can reject the null hypothesis and say there is an realtionship between X and Y. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the Accuracy of the Model\n",
    "\n",
    "The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the $R^2$ statistic.\n",
    "\n",
    "$RSE = \\sqrt{\\frac{1}{n-2}RSS} = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}{n}(y_i-\\hat{y_i})^2}$\n",
    "\n",
    "So the value from RSE is how much it deviats from the true regression line on average. an other ord for RSE is *lack of fit* for the model. So if the value of RSE is low, the model fits the data well.\n",
    "\n",
    "An other way to se *lack of fit* is $R^2$: \n",
    "\n",
    "$R^2 = 1-\\frac{RSS}{TSS}$ \n",
    "\n",
    "$TSS = \\sum(y_i - \\overline{y})^2$ - Total sum of squares\n",
    "\n",
    "$R^2$ is alwasy an value between 0 and 1. It takes the form of a proportion—the proportion of variance explained. An R2 statistic that is close to 1 indicates that a large proportion of the variability in the response is explained by the regression. A number near 0 indicates that the regression does not explain much of the variability in the response; this might occur because the linear model is wrong, or the error variance σ2 is high, or both.\n",
    "\n",
    "The significans of the value from $R^2$ is very much circumstantial. It depends on the application. **EX** physics it can be a strong relationship between the regression and the varibilty in the repsons, so a value closer to 1. But in an marketing application it can be larger amounte of residual errors and value closer to 0 is to be expected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Now we will use more then one varibal or feature to predict the value of Y. To understand the correlation between the diffrnt variabels we can look att an correlation matrix. Where values closer to 1 gives us an indecation that there is an correlation between variabels. \n",
    "\n",
    "Number of qustions that is improtent to ask is: \n",
    "\n",
    "### One: Is There a Relationship Between the Response and Predictors?\n",
    "\n",
    "Her we look at the null hypothesis agian. Using the F-statistic \n",
    "\n",
    "$F \\frac{(TSS-RSS)/p}{RSS(n-p-1)}$\n",
    "\n",
    "Where:\n",
    "- $p$ is number of variables, features.\n",
    "\n",
    "Here we look for an bigger value then 1 to reject the null hypothesis. When do we rejct null if the value is close to 1. It depands on $n$. If $n$ is small we need an larger F-statistic to reject null. And other way sround for a large $n$. \n",
    "\n",
    "### Two: Deciding on Important Variables\n",
    "\n",
    "There many ways to selct the best variables for predicting Y. One way is making diffrent models containing diffenrt number of variables and test against echother. This is not pratical in real life though. So there is three genral approches to this. \n",
    "\n",
    "- **Forward selctions** - Here we add in each variabel with the lowest RSS after an other, untill we reach an stopping rule. \n",
    "\n",
    "- **Backward selction** - Starting with all the variables and remove one by one with the largest p-value. This untill an stopping rule. \n",
    "\n",
    "-**Mixed selsction** - Keeping an eye on the p-values for the variables as we add them one by one. If one variable gose over an set value, we remove it. We continue doing this untill all the variables p-values are low enough. \n",
    "\n",
    "### Three: Model Fit\n",
    "\n",
    "$R^2$ and RSE is here again to tell us if the fit is good. Importent here is to test these two metrics on the traing data and test data. There is not nessecarly that an good value from the traing data gives an godd result from the test data. \n",
    "\n",
    "\n",
    "### Four: Predictions\n",
    "\n",
    "Using *Confidence interval* to quantify the uncertainty surrounding the avrge of Y for an larg amount of data points. The *prediction intervall* is focused on one data point and the interval contain the tru value of Y.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Considerations in the Regression Model\n",
    "\n",
    "## Qualitative Predictors\n",
    "\n",
    "If there is varibel in the data set that just gives an qualitative value. we might want to se if this qualitativ has any relationship with the estimation of Y. We assigne it a dummy variabel, that holds the value of qualiativ varibel as numeric repsentation. MOst often just 1 or 0. There is no limit to hown many dummy varibals we can make but givet just an binary result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Extensions of the Linear Model\n",
    "\n",
    "Standard linear model makes the assumption that there is an linearity respons to one unit of change in $X_j$ gives constant change in $Y$. Futher more the addative assumption is that between a predictor $X_j$ and the response $Y$ does not depend on the values of the other predictors.\n",
    "\n",
    "## Removing the addative assumption\n",
    "\n",
    "Given there is an synergy effect or rather an interaction effect. What we can do to account for this effect is to include an predictor, an interaction term. Now, we are stepping away from the addative assumption. \n",
    "\n",
    "$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2+ \\underline{\\beta_3X_1X_2} \\epsilon$\n",
    "\n",
    "If we do find an interaction term to be signifiknt, we should also include the associated variabels as independent varibals. So, $\\beta_3X_1X_2$ needs to have $\\beta_1X_1 + \\beta_2X_2$ in the model to work accordingly. It gives explanation to the model and the interaction.\n",
    "\n",
    "This can also be applied to a combination of quantitative and qualitative variables too. Given that we apply the interaction varibal for one of the values of the dummy variable. \n",
    "\n",
    "## Non-linear Relationships\n",
    "\n",
    "Here we can do polynomial regression. This is though a very costly method on many variables. Couse the incress in number of varibals is to the power of degree. Meaning that the dimensions of the model can get very large quickley. Also given large data sets, it just worsen this problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN compersing Linear Regression\n",
    "\n",
    "In short is that the more of an non-linear realtionship we have for the predictor and the respons, the better will KNN preform over linear regression. We can costimieze the Linear model to an point for an non-linear reltionship. but at one point the KNN will vastly outpreform the linear model. But limited to the number of dimensions for KNN. If there is many dimensions, the KNN model will become unusable. That is becous as dimansons increases the distans between the data points also gets longer, given the same data set. Called *curse of dimensionality*."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
