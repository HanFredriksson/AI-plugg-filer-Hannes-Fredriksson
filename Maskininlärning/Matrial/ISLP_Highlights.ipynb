{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archiev of higlights and excerpt from the book \"An Introduction to Statistical Learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction or Inference \n",
    "\n",
    "There are two main reasons that we may wish to estimate *f* : prediction\n",
    "and inference.\n",
    "\n",
    "### Prediction\n",
    "In many situations, a set of inputs X are readily available, but the output\n",
    "Y cannot be easily obtained. In this setting, since the error term averages\n",
    "to zero, we can predict Y using\n",
    "\n",
    "$\\hat{Y} = \\hat{f}(X)$\n",
    "\n",
    "Here we are bot concerned with the excat form of $\\hat{f}$, aslong it can accurately predict Y. The accuracy of ˆY as a prediction for Y depends on two quantities, which we will call the **reducible error** and the **irreducible error**.\n",
    "Reducabel error depends on the  statistical learning technique used. But the irreducible error are **unmeasured variable** or **unmeasurable variation**. . It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for Y . This bound is almost always unknown in practice.\n",
    "\n",
    "### Interference\n",
    "\n",
    "Now we want to now the exact form of $\\hat{f}$. Couse now the association between Y and $X_1, ...., X_n$ is neede to be understood. The following qustions should be asked:\n",
    "\n",
    "- Which predictors are associated with the response?\n",
    "- What is the relationship between the response and each predictor?\n",
    "- Can the relationship between Y and each predictor be adequately sum-\n",
    "marized using a linear equation, or is the relationship more compli-\n",
    "cated?\n",
    "\n",
    "So if you are looking for how the X is impacting the Y varibale and its values you are looking for an model that can find the interference. If you are looking for how x might affect an unknown Y you are looking for a model that make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do We Estimate f?\n",
    "\n",
    "Broadly speaking, most statistical learning methods for this task can be character-\n",
    "ized as either parametric or non-parametric.\n",
    "\n",
    "### Parametric Methods\n",
    "\n",
    "If we assume that $f(X)$ has a linear realtionsship we can use linear models estimate Y. Now we made it much easier to find the values of our predictors, coefficients or parameters. The issue is that the model can have to big of an error to estimate Y suficently. Ther is possible to pick an more flexibal model that can fit more functional forms for f. But the issue here is that the fexibal model can follow the errors, or noise, too closely. Leading to overfitting the model. \n",
    "\n",
    "### Non-Parametric Methods\n",
    "\n",
    "Now we do not assume the functional form of $f$. Instead they seek an estimate of $f$ that gets as close to the\n",
    "data points as possible. They have the potential to accurately fit a wider range of possible shapes for $f$. Because we do not assume any functional form for $f$ that could be wrong for the true $f$. One major disadvetage for an non-parametric model is that they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more\n",
    "than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$. In a parametric model we talk bout **smoothness of fit** to minimize the reducalbel error. But too much smoothness can overfit the model. \n",
    "\n",
    "### The Trade-Off Between Prediction Accuracy and Model Interpretability   \n",
    "\n",
    "We need to think about what want we need the model to do, is the interference or the prediction the most important. If we need to know the relations ship between parameters and the predicted Y, a more restrictive model is more useful. Because we get an easier way to interpret the parameters values and relationships. The more an model is fexible it becomes hrder to interpret the raltonsships of the parameters and predictions. \n",
    "\n",
    "### Supervised Versus Unsupervised Learning\n",
    "\n",
    "Is the difference between knowing (supervised) the respons value of $y_i$ for every $x_i$ or not knowing (unsupervised) the respons value of $y_i$. Each case needs a diffrent model to be used. \n",
    "\n",
    "### Regression Versus Classification Problems\n",
    "\n",
    "Variables can be characterized as either quantitative or qualitative (also known as categorical). Quantitative varibals are the things you can count or put a numerice value to. **EX:** The vlue of a house, age or height. Qualitative vaibals take value from number of classes or categories. **EX:** A brand, what diagnosis or person’s marital status. In generall the pick of model according to if the predictors are qualitative or quantitative is less important. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assessing Model Accuracy\n",
    "\n",
    "No model is best at every data set. We need to find the one that fits the best. \n",
    "\n",
    "## Measuring the Quality of Fit\n",
    "\n",
    "We need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. For regression MSE, mean squared error, measurement is used. In this case, we are looking for a small MSE value for a model to have an accurate prediction. \n",
    "\n",
    "In the interest of having an accurate prediction from a model, we want a low as possible **test MSE**. What we are interested in is to know whether $\\hat{f}$(x0) is approximately equal to $y_0$, where $(x_0, y_0)$ is a previously unseen test observation not used to train the statistical learning method. To find that model, we can use test data if available to evaluate the model.\n",
    "\n",
    "If there is no test data available, we can not just pick the model with lowest training MSE. There is small to no guarantee that that model will have low MSE for the test data. \n",
    "\n",
    "### Short about overfitting\n",
    "The higher flexibility the model has, the lower the training MSE is usually becoming, but most likely poorly fit the true $f$ and gives a higher MSE in the test data. Flexibility in a model can also be described as **Degrees of freedom**. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. Note that regardless of whether overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.\n",
    "\n",
    "## The Bias-Variance Trade-Off\n",
    "\n",
    "The expected test MSE can always be decomposed into the sum of three fundamental quantities: the variance of $\\hat{f}(x0)$, the squared bias of $\\hat{f}(x0)$ and the variance of the error variance bias terms $\\epsilon$\". In order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias. \n",
    "\n",
    "### Variance\n",
    "\n",
    "Variance refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set. So a small change in the training data set gives a small change in $\\hat{f}$, then we have a low variance. More flexible statistical methods have higher variance. \n",
    "\n",
    "### Bias \n",
    "\n",
    "Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. So, bias describes the relationship between the true $f$ and the $\\hat{f}$. The higher the bias, the further away from the true $f$ we are fitting our model. Generally, more flexible methods result in less bias.\n",
    "\n",
    "## Interpret Variance and Bias\n",
    "\n",
    "There is usually a sweet spot for where the relationship between variance and bias are optimal for the model picked to predict the data for the test data point. Looking at where the MSE increases and the value changes in variance and bias is much important. To find the omptimal set of values is called **bias-variance trade-of**. It is one of the biggest challenges and recurring problem in statistical learning. Takes time to be good at determine a good model from these values. \n",
    "\n",
    "## Quality of Fit in a Classification Setting\n",
    "\n",
    "The concepts of quality of fit for regression models also applies to a classification setting, with some modification. Here we talk about **training error rate,** the proportion of mistakes that are made if we apply error rate our estimate $\\hat{f}$ to the training observations. So this tells us if $\\hat{f}$ prediction $\\hat{y}$ is the same as Y classification. Again, we are more interested in the test error rate and a good classifier is one with low error rate. \n",
    "\n",
    "### The Bayes Classifier\n",
    "\n",
    "Very simple classifier that assigns each observation to the most likely class,\n",
    "given its predictor values. Bayes Classifier determines if $\\hat{y}$ is one or the other class though a calculate conditional of probability. Which category the response variable falls in are calculated from the **Bayes decision boundary**. Bayes error rate very low, but still has an unreducible error. To be able to use the Bayes Classifier, we need to know conditional distribution. \n",
    "\n",
    "### KNN, K-Nearest Neighbors \n",
    "\n",
    "When we do not know the conditional distribution, we use KNN. This method classifies any given observation to the class with the highest estimated probability. Given a positive integer K and a test observation $x_0$, the KNN classifier first identifies the K points in the training data that are closest to $x_0$, represented by $N_0$. It then estimates the conditional probability for class $j$ as the fraction of points in $N_0$ whose response values equal $j$. Even though KNN estimates the class, it has very good accuracy.\n",
    "\n",
    "In terms of quality of fit, the same applies as earlier. The error in training do not correspond to the amount of error in the test data. The same happens with bias and variance. As the flexibility decreases, higher K value, gives low bias and high variance. As before also have a tendency to overfit as the flexibility increases."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
